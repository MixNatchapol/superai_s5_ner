{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91251,"databundleVersionId":10905660,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install simpletransformers\nfrom simpletransformers.ner import NERModel, NERArgs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VALID_LABELS_DASH = {\n    \"O\",\n    \"B-ORG\",\n    \"B-PER\",\n    \"B-LOC\",\n    \"B-MEA\",\n    \"I-DTM\",\n    \"I-ORG\",\n    \"E-ORG\",\n    \"I-PER\",\n    \"B-TTL\",\n    \"E-PER\",\n    \"B-DES\",\n    \"E-LOC\",\n    \"B-DTM\",\n    \"B-NUM\",\n    \"I-MEA\",\n    \"E-DTM\",\n    \"E-MEA\",\n    \"I-LOC\",\n    \"I-DES\",\n    \"E-DES\",\n    \"I-NUM\",\n    \"E-NUM\",\n    \"B-TRM\",\n    \"B-BRN\",\n    \"I-TRM\",\n    \"E-TRM\",\n    \"I-TTL\",\n    \"I-BRN\",\n    \"E-BRN\",\n    \"E-TTL\",\n    \"B-NAME\"\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def underscore_to_dash(label: str) -> str:\n    \"\"\"\n    Convert a label with underscores (e.g. B_ORG) to dashes (B-ORG).\n    If it's not in the known list, default to \"O\".\n    \"\"\"\n    if label == \"O\":\n        return \"O\"\n    # Replace underscores with dashes\n    dashed_label = label.replace(\"_\", \"-\")\n    # If the dashed label is not in the valid set, set it to \"O\"\n    if dashed_label not in VALID_LABELS_DASH:\n        return \"O\"\n    return dashed_label\n\ndef get_sorted_txt_files(directory_path):\n    \"\"\"\n    Collect all .txt files under `directory_path`, sorted by filename.\n    \"\"\"\n    file_paths = []\n    for root, _, files in os.walk(directory_path):\n        for f in files:\n            if f.lower().endswith('.txt'):\n                file_paths.append(os.path.join(root, f))\n    file_paths.sort(key=lambda x: os.path.basename(x))\n    return file_paths\n\ndef parse_lst20_file(file_path, file_id, has_ner=True):\n    \"\"\"\n    Parse a single LST20 .txt file:\n      - has_ner=True => expect columns [token, pos, ner, clause]\n      - has_ner=False => expect columns [token, pos, clause]\n    We'll treat 'E_CLS' as a clause boundary => end of 'sentence'.\n\n    Return a list of \"sentences,\" each sentence = list of dict(token, pos, ner, clause).\n    \"\"\"\n    sentences = []\n    current_sentence = []\n\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            \n            parts = line.split()\n            \n            if has_ner:\n                # We expect 4 columns: token, pos, ner, clause\n                if len(parts) < 4:\n                    continue\n                token, pos_tag, ner_tag, clause_tag = parts[:4]\n                # Convert underscores to dashes, validate label\n                ner_tag = underscore_to_dash(ner_tag)\n            else:\n                # For test data (no NER), we expect 3 columns: token, pos, clause\n                if len(parts) < 3:\n                    continue\n                token, pos_tag, clause_tag = parts[:3]\n                ner_tag = \"O\"  # fallback label\n            \n            record = {\n                \"token\": token,\n                \"pos\": pos_tag,\n                \"ner\": ner_tag,\n                \"clause\": clause_tag\n            }\n            current_sentence.append(record)\n            \n            # If we see 'E_CLS', treat that as end of the current sentence.\n            if clause_tag == \"E_CLS\":\n                sentences.append(current_sentence)\n                current_sentence = []\n\n    if current_sentence:\n        sentences.append(current_sentence)\n\n    return sentences\n\ndef make_ner_dataframe(directory_path, split_name, has_ner=True):\n    \"\"\"\n    Build a DataFrame for Simple Transformers with columns:\n       sentence_id, words, labels\n\n    We treat each clause (ending with 'E_CLS') as one 'sentence'.\n    \"\"\"\n    files = get_sorted_txt_files(directory_path)\n    all_rows = []\n    sentence_id = 0\n\n    for file_path in files:\n        file_id = os.path.splitext(os.path.basename(file_path))[0]\n        list_of_sentences = parse_lst20_file(file_path, file_id, has_ner=has_ner)\n        \n        for sent in list_of_sentences:\n            for token_dict in sent:\n                # Convert \"ner\" to \"labels\" for Simple Transformers\n                row = {\n                    \"sentence_id\": sentence_id,\n                    \"words\": token_dict[\"token\"],   \n                    \"labels\": token_dict[\"ner\"],    \n                }\n                all_rows.append(row)\n            sentence_id += 1\n    \n    df = pd.DataFrame(all_rows)\n    print(f\"[{split_name}] => {len(df)} tokens, {sentence_id} sentences\")\n    return df\n\nif __name__ == \"__main__\":\n    dataset_path = \"/kaggle/input/super-ai-ss-5-named-entity-recognition\"\n    output_dir   = \"/kaggle/working\"\n\n    # Train\n    train_dir = os.path.join(dataset_path, \"train/train\")\n    train_df = make_ner_dataframe(train_dir, \"train\", has_ner=True)\n    train_df.to_csv(os.path.join(output_dir, \"train_preprocessed.csv\"), index=False, encoding='utf-8')\n\n    # Eval\n    eval_dir  = os.path.join(dataset_path, \"eval/eval\")\n    eval_df = make_ner_dataframe(eval_dir, \"eval\", has_ner=True)\n    eval_df.to_csv(os.path.join(output_dir, \"eval_preprocessed.csv\"), index=False, encoding='utf-8')\n    \n    # Test\n    test_dir = os.path.join(dataset_path, \"test/test\")\n    test_df = make_ner_dataframe(test_dir, \"test\", has_ner=False)\n    test_df.to_csv(os.path.join(output_dir, \"test_preprocessed.csv\"), index=False, encoding='utf-8')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom simpletransformers.ner import NERModel, NERArgs\nfrom sklearn.metrics import f1_score\n\nVALID_LABELS_DASH = [\n    \"O\",\n    \"B-ORG\",\n    \"B-PER\",\n    \"B-LOC\",\n    \"B-MEA\",\n    \"I-DTM\",\n    \"I-ORG\",\n    \"E-ORG\",\n    \"I-PER\",\n    \"B-TTL\",\n    \"E-PER\",\n    \"B-DES\",\n    \"E-LOC\",\n    \"B-DTM\",\n    \"B-NUM\",\n    \"I-MEA\",\n    \"E-DTM\",\n    \"E-MEA\",\n    \"I-LOC\",\n    \"I-DES\",\n    \"E-DES\",\n    \"I-NUM\",\n    \"E-NUM\",\n    \"B-TRM\",\n    \"B-BRN\",\n    \"I-TRM\",\n    \"E-TRM\",\n    \"I-TTL\",\n    \"I-BRN\",\n    \"E-BRN\",\n    \"E-TTL\",\n    \"B-NAME\"\n]\n\ndef macro_f1_metric(labels, preds, **kwargs):\n    flattened_labels = []\n    flattened_preds = []\n    for l_seq, p_seq in zip(labels, preds):\n        flattened_labels.extend(l_seq)\n        flattened_preds.extend(p_seq)\n\n    return {\"macro_f1\": f1_score(flattened_labels, flattened_preds, average=\"macro\")}\n\n\nif __name__ == \"__main__\":\n\n    # Confirm how many GPUs are visible:\n    n_gpus_available = torch.cuda.device_count()\n    print(f\"GPUs detected: {n_gpus_available}\")\n\n    # 1) Load data\n    train_df = pd.read_csv(\"train_preprocessed.csv\")\n    eval_df  = pd.read_csv(\"eval_preprocessed.csv\")\n\n    # 2) Set up model args\n    model_args = NERArgs()\n\n    model_args.num_train_epochs = 10\n    model_args.learning_rate = 2e-5\n    model_args.train_batch_size = 128\n    model_args.gradient_accumulation_steps = 2\n    model_args.eval_batch_size = 128\n    \n    # Evaluate each epoch\n    model_args.evaluate_during_training = True\n    \n    # Minimal checkpointing\n    model_args.save_model_every_epoch = False\n    model_args.save_eval_checkpoints = False\n    model_args.save_steps = -1\n    model_args.overwrite_output_dir = True\n    model_args.save_best_model = True\n    model_args.save_optimizer_and_scheduler = False\n    \n    # Speed ups\n    model_args.logging_steps = 100\n    model_args.reprocess_input_data = False\n    model_args.use_cached_eval_features = True\n    \n    # Provide label list\n    model_args.labels_list = VALID_LABELS_DASH","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom simpletransformers.ner import NERModel\nfrom tqdm import tqdm\n\n######################################\n# 1. Define the label-to-ID mapping\n######################################\nlabel2id = {\n    \"O\": 0,\n    \"B-ORG\": 1,\n    \"B-PER\": 2,\n    \"B-LOC\": 3,\n    \"B-MEA\": 4,\n    \"I-DTM\": 5,\n    \"I-ORG\": 6,\n    \"E-ORG\": 7,\n    \"I-PER\": 8,\n    \"B-TTL\": 9,\n    \"E-PER\": 10,\n    \"B-DES\": 11,\n    \"E-LOC\": 12,\n    \"B-DTM\": 13,\n    \"B-NUM\": 14,\n    \"I-MEA\": 15,\n    \"E-DTM\": 16,\n    \"E-MEA\": 17,\n    \"I-LOC\": 18,\n    \"I-DES\": 19,\n    \"E-DES\": 20,\n    \"I-NUM\": 21,\n    \"E-NUM\": 22,\n    \"B-TRM\": 23,\n    \"B-BRN\": 24,\n    \"I-TRM\": 25,\n    \"E-TRM\": 26,\n    \"I-TTL\": 27,\n    \"I-BRN\": 28,\n    \"E-BRN\": 29,\n    \"E-TTL\": 30,\n    \"B-NAME\": 31\n}\n\n######################################\n# 2. Load data function with sorted filenames ~> not optimal, there is a better way\n######################################\ndef load_data_to_df(data_folder, is_train=True):\n    rows = []\n    global_sentence_id = 0\n\n    # Get sorted list of filenames\n    sorted_filenames = sorted(f for f in os.listdir(data_folder) if f.endswith(\".txt\"))\n\n    for fname in sorted_filenames:\n        filepath = os.path.join(data_folder, fname)\n        file_id = os.path.splitext(fname)[0]\n        line_idx = 0\n\n        words_buffer = []\n        labels_buffer = []\n        line_indices = []\n\n        with open(filepath, 'r', encoding='utf-8') as f:\n            for raw_line in f:\n                line_str = raw_line.strip()\n\n                if not line_str:\n                    if words_buffer:\n                        for i, (w, lab) in enumerate(zip(words_buffer, labels_buffer)):\n                            rows.append({\n                                \"filename\": file_id,\n                                \"line_index\": line_indices[i],\n                                \"sentence_id\": global_sentence_id,\n                                \"words\": w,\n                                \"labels\": lab\n                            })\n                        global_sentence_id += 1\n                        words_buffer.clear()\n                        labels_buffer.clear()\n                        line_indices.clear()\n                    line_idx += 1\n                    continue\n\n                parts = line_str.split('\\t')\n                if len(parts) < 3:\n                    line_idx += 1\n                    continue\n\n                token, pos, ner_tag = parts[:3]\n                words_buffer.append(token)\n                labels_buffer.append(ner_tag)\n                line_indices.append(line_idx)\n                line_idx += 1\n\n            if words_buffer:\n                for i, (w, lab) in enumerate(zip(words_buffer, labels_buffer)):\n                    rows.append({\n                        \"filename\": file_id,\n                        \"line_index\": line_indices[i],\n                        \"sentence_id\": global_sentence_id,\n                        \"words\": w,\n                        \"labels\": lab\n                    })\n                global_sentence_id += 1\n\n    return pd.DataFrame(rows)\n\n######################################\n# 3. Segment Clauses\n######################################\ndef segment_clauses(test_df_sorted):\n    grouped_sentences = (\n        test_df_sorted.groupby(\"sentence_id\")[\"words\"]\n        .apply(list)\n        .reset_index()\n        .rename(columns={\"words\": \"tokens\"})\n    )\n    return grouped_sentences\n\n######################################\n# 4. Prediction and Alignment\n######################################\ndef predict_and_align(model, segmented_clauses, batch_size=64):\n    predictions = []\n    for tokens in tqdm(segmented_clauses[\"tokens\"], desc=\"Running Predictions\"):\n        try:\n            subword_predictions, _ = model.predict([tokens], split_on_space=False)\n            predictions.extend(subword_predictions[0])\n        except Exception as e:\n            print(f\"Error during prediction for tokens: {tokens[:10]}...: {e}\")\n            predictions.extend([\"O\"] * len(tokens))\n    return predictions\n\n######################################\n# 5. Main Execution\n######################################\ndataset_path = \"/kaggle/input/super-ai-ss-5-named-entity-recognition/\"\ntest_dir = os.path.join(dataset_path, \"test/test\")\nsample_submission_path = os.path.join(dataset_path, \"sample_submission.csv\")\n\n######################################\n# Main Execution for Full Data\n######################################\n\n# Load full test data\nprint(\"Loading full test data...\")\ntest_df = load_data_to_df(test_dir, is_train=False)\nprint(f\"\\nLoaded Full Test Data: {len(test_df)} rows\")\n\n# Sort test data\ntest_df_sorted = test_df.sort_values(by=['filename', 'sentence_id', 'line_index']).reset_index(drop=True)\ntest_df_sorted['id'] = test_df_sorted.apply(\n    lambda row: f\"{row['filename']}_{row['line_index']}\", axis=1\n)\n\n# Print the head of test_df_sorted for verification\nprint(\"\\nHead of test_df_sorted before segmentation:\")\nprint(test_df_sorted.head())\n\n# Segment clauses\nsegmented_clauses = segment_clauses(test_df_sorted)\n\n# Predict on full data\nprint(\"\\nRunning predictions on full dataset...\")\npredictions = predict_and_align(model, segmented_clauses, batch_size=64)\n\n# Map predictions to numerical values\npredicted_ne = []\nfor pred in predictions:\n    if isinstance(pred, dict):\n        label = list(pred.values())[0]  # Extract the label from the dictionary\n    else:\n        label = pred\n    predicted_ne.append(label2id.get(label, 0))  # Map the label to its numerical ID\n\n# Add predicted labels to test_df_sorted\ntest_df_sorted['ne'] = predicted_ne[:len(test_df_sorted)]  # Ensure alignment\n\n# Drop the 'words' column and prepare submission DataFrame\nsubmission_df = test_df_sorted[['id', 'ne']].copy()\n\n# Save to submission.csv\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"\\n'submission.csv' has been created successfully.\")\n\n# Check the format of the output file\nprint(\"\\nHead of submission.csv:\")\nprint(submission_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom simpletransformers.ner import NERModel\nfrom tqdm import tqdm\n\n######################################\n# 1. Define the label-to-ID mapping\n######################################\nlabel2id = {\n    \"O\": 0,\n    \"B-ORG\": 1,\n    \"B-PER\": 2,\n    \"B-LOC\": 3,\n    \"B-MEA\": 4,\n    \"I-DTM\": 5,\n    \"I-ORG\": 6,\n    \"E-ORG\": 7,\n    \"I-PER\": 8,\n    \"B-TTL\": 9,\n    \"E-PER\": 10,\n    \"B-DES\": 11,\n    \"E-LOC\": 12,\n    \"B-DTM\": 13,\n    \"B-NUM\": 14,\n    \"I-MEA\": 15,\n    \"E-DTM\": 16,\n    \"E-MEA\": 17,\n    \"I-LOC\": 18,\n    \"I-DES\": 19,\n    \"E-DES\": 20,\n    \"I-NUM\": 21,\n    \"E-NUM\": 22,\n    \"B-TRM\": 23,\n    \"B-BRN\": 24,\n    \"I-TRM\": 25,\n    \"E-TRM\": 26,\n    \"I-TTL\": 27,\n    \"I-BRN\": 28,\n    \"E-BRN\": 29,\n    \"E-TTL\": 30,\n    \"B-NAME\": 31\n}\n\n######################################\n# 2. Load data function with sorted filenames Thank you from Ouh\n######################################\ndef load_data_to_df(data_folder, is_train=True):\n    rows = []\n    global_sentence_id = 0\n\n    # Get sorted list of filenames\n    sorted_filenames = sorted(f for f in os.listdir(data_folder) if f.endswith(\".txt\"))\n\n    for fname in sorted_filenames:\n        filepath = os.path.join(data_folder, fname)\n        file_id = os.path.splitext(fname)[0]\n        line_idx = 0\n\n        words_buffer = []\n        labels_buffer = []\n        line_indices = []\n\n        with open(filepath, 'r', encoding='utf-8') as f:\n            for raw_line in f:\n                line_str = raw_line.strip()\n\n                if not line_str:\n                    if words_buffer:\n                        for i, (w, lab) in enumerate(zip(words_buffer, labels_buffer)):\n                            rows.append({\n                                \"filename\": file_id,\n                                \"line_index\": line_indices[i],\n                                \"sentence_id\": global_sentence_id,\n                                \"words\": w,\n                                \"labels\": lab\n                            })\n                        global_sentence_id += 1\n                        words_buffer.clear()\n                        labels_buffer.clear()\n                        line_indices.clear()\n                    line_idx += 1\n                    continue\n\n                parts = line_str.split('\\t')\n                if len(parts) < 3:\n                    line_idx += 1\n                    continue\n\n                token, pos, ner_tag = parts[:3]\n                words_buffer.append(token)\n                labels_buffer.append(ner_tag)\n                line_indices.append(line_idx)\n                line_idx += 1\n\n            if words_buffer:\n                for i, (w, lab) in enumerate(zip(words_buffer, labels_buffer)):\n                    rows.append({\n                        \"filename\": file_id,\n                        \"line_index\": line_indices[i],\n                        \"sentence_id\": global_sentence_id,\n                        \"words\": w,\n                        \"labels\": lab\n                    })\n                global_sentence_id += 1\n\n    return pd.DataFrame(rows)\n\n######################################\n# 3. Segment Clauses\n######################################\ndef segment_clauses(test_df_sorted):\n    grouped_sentences = (\n        test_df_sorted.groupby(\"sentence_id\")[\"words\"]\n        .apply(list)\n        .reset_index()\n        .rename(columns={\"words\": \"tokens\"})\n    )\n    return grouped_sentences\n\n######################################\n# 4. Prediction and Alignment\n######################################\ndef predict_and_align(model, segmented_clauses, batch_size=64):\n    predictions = []\n    for tokens in tqdm(segmented_clauses[\"tokens\"], desc=\"Running Predictions\"):\n        try:\n            subword_predictions, _ = model.predict([tokens], split_on_space=False)\n            predictions.extend(subword_predictions[0])\n        except Exception as e:\n            print(f\"Error during prediction for tokens: {tokens[:10]}...: {e}\")\n            predictions.extend([\"O\"] * len(tokens))\n    return predictions\n\n######################################\n# 5. Main Execution\n######################################\ndataset_path = \"/kaggle/input/super-ai-ss-5-named-entity-recognition/\"\ntest_dir = os.path.join(dataset_path, \"test/test\")\nsample_submission_path = os.path.join(dataset_path, \"sample_submission.csv\")\n\n######################################\n# Main Execution for Full Data\n######################################\n\n# Load full test data\nprint(\"Loading full test data...\")\ntest_df = load_data_to_df(test_dir, is_train=False)\nprint(f\"\\nLoaded Full Test Data: {len(test_df)} rows\")\n\n# Sort test data\ntest_df_sorted = test_df.sort_values(by=['filename', 'sentence_id', 'line_index']).reset_index(drop=True)\ntest_df_sorted['id'] = test_df_sorted.apply(\n    lambda row: f\"{row['filename']}_{row['line_index']}\", axis=1\n)\n\n# Print the head of test_df_sorted for verification\nprint(\"\\nHead of test_df_sorted before segmentation:\")\nprint(test_df_sorted.head())\n\n# Segment clauses\nsegmented_clauses = segment_clauses(test_df_sorted)\n\n# Initialize model\nmodel = NERModel(\n    model_type=\"xlmroberta\",\n    model_name=\"/kaggle/working/best_model\",\n    labels=list(label2id.keys()),\n    args={\"max_seq_length\": 512},\n    use_cuda=True\n)\n\n# Predict on full data\nprint(\"\\nRunning predictions on full dataset...\")\npredictions = predict_and_align(model, segmented_clauses, batch_size=64)\n\n# Map predictions to numerical values\npredicted_ne = []\nfor pred in predictions:\n    if isinstance(pred, dict):\n        label = list(pred.values())[0]  # Extract the label from the dictionary\n    else:\n        label = pred\n    predicted_ne.append(label2id.get(label, 0))  # Map the label to its numerical ID\n\n# Add predicted labels to test_df_sorted\ntest_df_sorted['ne'] = predicted_ne[:len(test_df_sorted)]  # Ensure alignment\n\n# Drop the 'words' column and prepare submission DataFrame\nsubmission_df = test_df_sorted[['id', 'ne']].copy()\n\n# Save to submission.csv\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"\\n'submission.csv' has been created successfully.\")\n\n# Check the format of the output file\nprint(\"\\nHead of submission.csv:\")\nprint(submission_df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}